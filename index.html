<!DOCTYPE html>
<html>
  <head>
    <title>SDN in the world of OpenStack</title>
    <meta charset="utf-8">
    <link rel="stylesheet" href="base_template.css" />
    <link rel="stylesheet" href="style.css" />
  </head>
  <body>
    <textarea id="source">

name: title
class: center, middle
background-image: url(images/title_background.png)
background-size: 908.21px 681px
background-position: 50% 0

# SDN in the world of OpenStack

.presenter[
Sławek Kapłoński  
slawomir.kaplonski@ovh.net  
IRC: slaweq
]

---
name: agenda

# Agenda

1. What is SDN?
2. What is Openstack?
  * Openstack in general
  * Main components
3. How SDN is done in Openstack
  * Basic concept of Openstack Neutron
  * Neutron's main components
4. Example of connections scenario made by Openstack Neutron
  * legacy scenario with Openvswitch and Network node

---

name: sdn_definition

# What is SDN?

**Software-defined networking (SDN)** is an approach to computer networking that
allows network administrators to programmatically initialize, control, change,
and manage network behavior dynamically via open interfaces and abstraction
of lower-level functionality.  
.source_txt[Source: [wikipedia](https://en.wikipedia.org/wiki/Software-defined_networking)]

.left_side_img[
    ![onf_logo](images/onf-logo.png)
]
.right_side_img[
    ![openflow_logo](images/OpenFlow-Logo-Small.jpg)
]

???
Definicja SDN (wg np. wikipedia): **Software Defined Networking**
Można spotkać się też z określeninem że jest to **programowalna sieć**  
Ogólnie chodzi o to, że  SDN to podejście to sieci komputerowych, które pozwala
na programowalną kontrolę, zmianę i zarządzanie siecią z jednego
scentralizowanego punktu przez jakiś interfejs, który jest abstrakcją
przykrywającą jakieś specyficzne implementacje.

---

name: sdn_definition_2

# What is SDN?

.full_slide_img[
    ![sdn_diagram](images/sdn-3layers.gif)
]
.source_txt[source: https://www.opennetworking.org]

???

W modelu SDN można wyróżnić 3 warstwy:  
* warstwę aplikacji, która komunikuje się z warstwą zarządzającą SDN'em przez API
* warstwę która udostępnia API i zarządza całym SDN'em - jest ona scentralizowana
* warstwę infrastrukturalną (tu znajdują się urządzenia sieciowe) - najczęściej
  do komunikacji pomiędzy kontrolerem a urządzeniami sieciowymi wykorzystywany
  jest protokół **Openflow**

Dzięki takiemu podejściu aplikacja/user może w łatwy sposób zmieniać topologię
takiej sieci SDN.

---

name: openstack_general
class: split-40

# What is Openstack?

.column[
![openstack_logo](images/OpenStack-Logo-Vertical.png)  
.source_txt[source: http://www.openstack.org]
]
.column[
    OpenStack is a cloud operating system that controls large pools of compute,
    storage, and networking resources throughout a datacenter, all managed
    through a dashboard that gives administrators control while empowering their
    users to provision resources through a web interface.
]


???
Ogólna definicja ze strony Openstack Foundation:  
  
Openstack to system, który kontroluje duże pule mocy obliczeniowej, storage'u i
zasobów sieciowych w całym DC. Wszystko zarządzane jest przez jeden dashboard
(oraz API), który umożliwia kontrolę administratorowi i daje możliwość
korzystania z tych zasobów użytkownikom.

---

name: openstack_main_components
class: center

# Base Openstack components

![openstack_architecture](images/openstack-software-diagram.png)  
.source_txt[source: http://www.openstack.org/software]

???
Openstack'a można podzielić na 3 podstawowe części:
* compute - Nova (VMs), Glance (Images)
* storage - Swift (object storage), Cinder (block storage)
* networking - Neutron

Jest również wiele innych, np.:
* Horizon (dashboard),
* keystone (autentykacja),
* wiele innych jak: ceilometer (telemetry) or kuryr (networking for containers)

Wsystkie te projekty dostarczają REST API do zarządzania/obsługi Openstack'a

--

name: openstack_main_components

.left_side_list[
* compute - Nova (VMs), Glance (Images)
]

--

name: openstack_main_components

.left_side_list[
* storage - Swift (object storage), Cinder (block storage)
]

--

name: openstack_main_components

.left_side_list[
* networking - Neutron
]

---

name: neutron_main_components
class: center

# Neutron main components

![architecture](images/neutron_components_architecture.png)

???
Neutron podzielony jest na kilka głównych komponentów:

* neutron server - zapewnia obsługę API, ma dostęp do bazy,
* tzw. L2 agenty - agenty działające na wszystkich node'ach w klastrze -
  zapewniają podłączenie "portów" do sieci
* inne agenty takie jak DHCP agent, L3 agent czy metadata agent

Taka struktura (z agentami) dotyczy Neutrona z tzw. ML2 plugin'em, który jest
domyślną referencyjną implementacją.
Istnieją inne core pluginy, dostarczane przez firmy trzecie, np. Midonet, Calico
czy PLUMgrid. Mogą one działać w inny sposób, np. bez L2 czy DHCP agentów.

Komunikacja pomiędzy Neutron serverem i agentami odbywa się przez RPC (typowo
rabbitmq).

---

name: neutron_as_sdn
class: center

# Neutron as SDN

![neutron_sdn_diagram](images/neutron-sdn-3layers.png)

???
Diagram modelu SDN ale z zaznaczonymi komponentami Openstack'a w każdej z warstw.
* Warstwa aplikacji - taką aplikacją korzystającą z SDN'a może być np.
  Nova-compute, która tworzy instancje i oczekuje od Neutron'a, aby były one
  podpięte do konkretnej sieci. Taka aplikacja komunikuje się z SDN'em
  (neutron'em) przez REST Api jakie udostępnia Neutron.
* Zarządzaniem całego SDN'a i obsługą API zajmuje się neutron server. To on
  jako jedyny ma dostęp do bazy danych i posiada informacje o wszystkich
  działających w sieci agentach, dostępnych typach sieci i driver'ach. Na tej
  podstawie może przydzielić (albo nie) port do podłączenia np. VMki do sieci
  prywatnej lub publicznej.
* Warstwa infrastruktury to cały klaster serwerów compute z L2 agentami i tzw.
  network node'y (z L2 i L3 agentami) realizujące usługi sieciowe, np. vrouter
  czy fwaas

Inaczej niż w typowym SDN'ie komunikacja pomiędzy warstwą zarządzającą a
infrastrukturą (agentami) odbywa się przez protoków RPC a nie Openflow.
Openflow jest wykorzystywany np. przez Openvswitch L2 agenty, ale jest to tylko
jedna z możliwych do wykorzystania technologii.

---

name: neutron_usage_example_create_network
class: center

# How to use Neutron

.left_side_list[
create network
]

```
admin@devstack-2:~$ neutron net-create Local-Network
Created a new network:
+---------------------------+--------------------------------------+
| Field                     | Value                                |
+---------------------------+--------------------------------------+
| admin_state_up            | True                                 |
| availability_zone_hints   |                                      |
| availability_zones        |                                      |
| created_at                | 2016-08-01T12:38:36                  |
| description               |                                      |
| id                        | 74149489-5cd6-473e-aeb7-d003652c8048 |
| ipv4_address_scope        |                                      |
| ipv6_address_scope        |                                      |
| mtu                       | 0                                    |
| name                      | Local-Network                        |
| port_security_enabled     | True                                 |
| provider:network_type     | local                                |
| provider:physical_network |                                      |
| provider:segmentation_id  |                                      |
```

???
Pierwsze co należy zrobić w Neutronie to utworzyć sieć.  
Sieć jest to jakiś izolowany segment. Można porównać to np. do VLANu w sieci
fizycznej.
Mogą być sieci typu:
* tenant network - sieci prywatne, per tenant - domyślnie realizowane przez tunele vxlan lub gre
* provider network - sieć mapowana bezpośrednio na fizyczną sieć w DC, np. vlan albo flat network
* external network - taka jak provider network ale zapewniająca również dostęp do internetu

---

name: neutron_usage_example_create_subnet
class: center

# How to use Neutron

.left_side_list[
create subnet
]


```
admin@devstack-2:~$ neutron subnet-create Local-Network 10.0.0.0/24
Created a new subnet:
+-------------------+--------------------------------------------+
| Field             | Value                                      |
+-------------------+--------------------------------------------+
| allocation_pools  | {"start": "10.0.0.2", "end": "10.0.0.254"} |
| cidr              | 10.0.0.0/24                                |
| created_at        | 2016-08-01T12:47:28                        |
| description       |                                            |
| dns_nameservers   |                                            |
| enable_dhcp       | True                                       |
| gateway_ip        | 10.0.0.1                                   |
| host_routes       |                                            |
| id                | 9f31692e-f2b8-4db0-9073-1c732373b9af       |
| ip_version        | 4                                          |
| ipv6_address_mode |                                            |
| ipv6_ra_mode      |                                            |
| name              |                                            |
| network_id        | 74149489-5cd6-473e-aeb7-d003652c8048       |
| subnetpool_id     |                                            |
```

???
Następnym krokiem jest utworzenie tzw. subnet'u.  
Subnet jest to block adresów IP razem z całą konfiguracją (gw, dhcp server IP, IP range)

---

name: neutron_usage_example_create_port
class: center

# How to use Neutron

.left_side_list[
create port
]

```
admin@devstack-2:~$ neutron port-create Local-Network --binding:host_id devstack-2
+-----------------------+------------------------------------------------+
| Field                 | Value                                          |
+-----------------------+------------------------------------------------+
| admin_state_up        | True                                           |
| binding:host_id       | devstack-2                                     |
| binding:vif_details   | {"port_filter": true, "ovs_hybrid_plug": true} |
| binding:vif_type      | ovs                                            |
| binding:vnic_type     | normal                                         |
| created_at            | 2017-03-03T21:08:04Z                           |
| fixed_ips             | {"subnet_id": "...", "ip_address": "10.0.0.5"} |
| id                    | 269cc404-5fb2-465a-94dc-76a17b3781dd           |
| mac_address           | fa:16:3e:6e:f8:9f                              |
| network_id            | 45b19f14-e531-431b-b66a-755d8d64ba17           |
| port_security_enabled | True                                           |
| project_id            | 4ec2efced54b44719377a710aacbf3da               |
| revision_number       | 6                                              |
| status                | DOWN                                           |

```

???
Gdy sieć i subnet mamy już gotowy, można utworzyć port. Najczęściej robi to za
nas Nova tworząc VMkę, ale można to zrobić również samemu.
Aby port został skonfigurowany na hoście, musimy go tam odpowiednio podłączyć,
np. w przypadku bindingu "ovs" trzeba dodać interfejs o konkretnej nazwie do
bridge'a integracji na hoście. To też w przypadku Openstacka robi nova-compute.

Gdy port jest już dodany do bridge'a, resztę konfiguracji na hoście robi L2
agent (ovs w tym wypadku)

---

name: example_scenario_1
class: center, scenario

#Network scenario example

![scenario overview](images/scenario-legacy-general.png)

???
To teraz jak już port jest utworzony to spróbujmy prześledzić co Neutron robi z
tym portem i jak ten ruch trafia do i z tego portu

Na początek przykładowy klaster Openstacka, który będziemy analizować:
* network node - węzeł realizujący usługi 3 warstwy (vrouter, floating ip), usługa DHCP
* compute node'y - serwery w klastrze

Przedstawiony przykład obejmuje scenariusz z ovs agentem jako L2 agent i tzw.
klasyczny network-node (są warianty z L3-HA również i z DVR'em)

---

name: example_scenario_2
class: center, scenario

#Network scenario example

.left_side_img[
![scenario compute 1](images/scenario-legacy-ovs-compute1.png)
]
.right_side_img[
![scenario compute 2](images/scenario-legacy-ovs-compute2.png)
]

???
Compute node. Po lewej ogólnie co się na nim znajduje:
instancja,
Linuxbridge, który realizuje tzw. security grupy
ovs z bridgami integracji (wpięte do niego są instancje), tunnelu i vlanowy

Po prawej stronie już bardziej szczegółowo jak instancja jest podłączona do sieci L2:
eth0 instancji połączone jest z interfejsem tap który jest portem w bridgu linuksowym
następnie mamy parę veth’ów z których jeden koniec zbridgowany z tap’em a drugi
jest umieszony w br integracji;
Mogło by to być zrobione prościej i tap mógłby być bezpośrednio w bridgu
integracji ale wtedy SG (realizowane przez iptables) nie działały by dla tego
portu wcale, następnie bridge integracji połączony jest z bridgami tunnelu i vlan, dla
każdego portu są tam regułki openflow które decydują czy pakiety z takiego tap’a
mają trafić do br-tun czy vlan

---

name: example_scenario_3
class: center, scenario

#Network scenario example

.left_side_img[
![scenario network 1](images/scenario-legacy-ovs-network1.png)
]
.right_side_img[
![scenario network 2](images/scenario-legacy-ovs-network2.png)
]

???

Network node. Rysunek po lewej bardzo podobny do tego z compute node’a. Oprócz
instancji jest L3 agent i DHCP agent i dodatkowo w ovs’ie jest br-ex który służy
do podłączenia sieci do internetu.

Na rysunku po prawej przedstawiony jest schemat połączeń na takim network nodzie.
Po pierwsze jest namespace dhcp (qdhcp) i interfej tap serwera dhcp jest
bezpośrednio wpięty do bridge’a integracji.
Z drugiej strony namespace qrouter w którym są porty qr i qg czyli połączenie
routera do sieci prywatnej (qr) i do internetu (qg). Te porty również są w
bridge’u integracji.
Reguły openflow kierują później pakiety do odpowiednich bridge’y (br-tun, br-ex,
vlan)

---

name: example_scenario_4
class: center, scenario

#Network scenario example

.full_slide_img[
![scenario flows](images/scenario-legacy-ovs-flowns1-compute.png)
]

???
Przykładowy przepływ pakietów z instancji do internetu (czyli tzw. ruch
north/south).

Pakiety z instacnji przechodzą przez qbr i trafiają do br-int’a a następnie
przez patch-tun lub int-br-vlan do odpowiedniego bridge’a. W bridge’ach tych
kolejne reguły openflow mogą zmienić np. VLAN tag pakietu lub dla sieci vxlan
„opakować” pakiet w odpowiedni VNI.
---

name: example_scenario_5
class: center, scenario

#Network scenario example

.image-w80pr[
![scenario flows](images/scenario-legacy-ovs-flowns1-network.png)
]

???

Pakiety trafiają do network node’a i przechodzą podobną drogę: przez bridge tun
lub vlan i patch port do bridge’a integracji trafiają do router’a (namespace’a)
do portu qr. Tam są NAT’owane na adres gateway’a (ustawiony na qg) i wysyłane w
świat.

---

class: last

# Questions?

.presenters[
Sławek Kapłoński  
slawomir.kaplonski@ovh.net  
IRC: slaweq
]

## Do You want to build Openstack clouds?

.job_link[
ovh.pl/jobs - Openstack Cloud Engineer
]

## Reference

* https://wiki.openstack.org/wiki/Neutron
* https://www.opennetworking.org/
* http://slawqo.github.io/neutron_openstack_sdn/


    </textarea>
    <script src="remark-latest.min.js">
    </script>
    <script>
        var slideshow = remark.create({ratio: '16:9',
                                       countIncrementalSlides: false});
    </script>

  </body>
</html>
